apiVersion: v1
kind: ConfigMap
metadata:
  name: production-deployment-config
  namespace: nexus-infrastructure
data:
  production_checklist.yaml: |
    # Nexus Architect Production Readiness Checklist
    
    infrastructure:
      kubernetes:
        - cluster_version: ">=1.25"
        - node_count: ">=3"
        - resource_quotas: "configured"
        - network_policies: "enabled"
        - rbac: "configured"
        - monitoring: "enabled"
        - logging: "centralized"
        - backup: "automated"
      
      storage:
        - persistent_volumes: "configured"
        - backup_strategy: "implemented"
        - encryption: "enabled"
        - retention_policy: "defined"
      
      networking:
        - load_balancer: "configured"
        - ssl_certificates: "valid"
        - dns: "configured"
        - firewall_rules: "implemented"
    
    security:
      authentication:
        - oauth_configured: true
        - mfa_enabled: true
        - session_management: true
        - password_policy: true
      
      authorization:
        - rbac_implemented: true
        - api_security: true
        - data_access_controls: true
        - audit_logging: true
      
      encryption:
        - data_at_rest: true
        - data_in_transit: true
        - key_management: true
        - certificate_management: true
      
      compliance:
        - gdpr_compliance: true
        - soc2_compliance: true
        - hipaa_compliance: true
        - security_scanning: true
    
    performance:
      optimization:
        - caching_strategy: "implemented"
        - database_optimization: "completed"
        - cdn_configuration: "enabled"
        - compression: "enabled"
      
      monitoring:
        - metrics_collection: "comprehensive"
        - alerting: "configured"
        - dashboards: "operational"
        - log_aggregation: "centralized"
      
      scalability:
        - horizontal_scaling: "configured"
        - auto_scaling: "enabled"
        - load_testing: "completed"
        - capacity_planning: "documented"
    
    reliability:
      availability:
        - high_availability: "99.9%"
        - disaster_recovery: "tested"
        - backup_restoration: "verified"
        - failover_procedures: "documented"
      
      testing:
        - unit_tests: ">=90% coverage"
        - integration_tests: "comprehensive"
        - end_to_end_tests: "automated"
        - performance_tests: "baseline_established"
      
      deployment:
        - blue_green_deployment: "configured"
        - rollback_procedures: "tested"
        - health_checks: "comprehensive"
        - deployment_automation: "implemented"
    
    operations:
      documentation:
        - architecture_documentation: "complete"
        - operational_procedures: "documented"
        - troubleshooting_guides: "available"
        - api_documentation: "comprehensive"
      
      support:
        - incident_response: "procedures_defined"
        - escalation_matrix: "documented"
        - on_call_rotation: "established"
        - knowledge_base: "maintained"
      
      maintenance:
        - update_procedures: "automated"
        - security_patching: "scheduled"
        - database_maintenance: "automated"
        - log_rotation: "configured"
  
  deployment_procedures.sh: |
    #!/bin/bash
    
    # Nexus Architect Production Deployment Procedures
    
    set -euo pipefail
    
    # Configuration
    NAMESPACE="nexus-production"
    BACKUP_RETENTION_DAYS=30
    HEALTH_CHECK_TIMEOUT=300
    ROLLBACK_TIMEOUT=600
    
    # Colors for output
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color
    
    # Logging function
    log() {
        echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
    }
    
    warn() {
        echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
    }
    
    error() {
        echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    }
    
    # Pre-deployment checks
    pre_deployment_checks() {
        log "Starting pre-deployment checks..."
        
        # Check Kubernetes cluster health
        if ! kubectl cluster-info &>/dev/null; then
            error "Kubernetes cluster is not accessible"
            exit 1
        fi
        
        # Check namespace exists
        if ! kubectl get namespace $NAMESPACE &>/dev/null; then
            log "Creating namespace $NAMESPACE"
            kubectl create namespace $NAMESPACE
        fi
        
        # Check required secrets exist
        local required_secrets=("postgresql-secrets" "redis-secrets" "vault-secrets" "ssl-certificates")
        for secret in "${required_secrets[@]}"; do
            if ! kubectl get secret $secret -n $NAMESPACE &>/dev/null; then
                error "Required secret $secret not found in namespace $NAMESPACE"
                exit 1
            fi
        done
        
        # Check storage classes
        if ! kubectl get storageclass fast-ssd &>/dev/null; then
            warn "Fast SSD storage class not found, using default"
        fi
        
        # Check resource quotas
        local cpu_limit=$(kubectl get resourcequota -n $NAMESPACE -o jsonpath='{.items[0].spec.hard.requests\.cpu}' 2>/dev/null || echo "none")
        local memory_limit=$(kubectl get resourcequota -n $NAMESPACE -o jsonpath='{.items[0].spec.hard.requests\.memory}' 2>/dev/null || echo "none")
        
        log "Resource limits - CPU: $cpu_limit, Memory: $memory_limit"
        
        log "Pre-deployment checks completed successfully"
    }
    
    # Database backup
    backup_database() {
        log "Creating database backup..."
        
        local backup_name="nexus-backup-$(date +%Y%m%d-%H%M%S)"
        
        # Create backup job
        kubectl apply -f - <<EOF
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: $backup_name
      namespace: $NAMESPACE
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: backup
            image: postgres:15
            env:
            - name: PGHOST
              value: "postgresql.$NAMESPACE"
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgresql-secrets
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgresql-secrets
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              pg_dump -h \$PGHOST -U \$PGUSER nexus_architect > /backup/\$backup_name.sql
              echo "Backup completed: \$backup_name.sql"
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
    EOF
        
        # Wait for backup to complete
        kubectl wait --for=condition=complete job/$backup_name -n $NAMESPACE --timeout=600s
        
        log "Database backup completed: $backup_name"
        
        # Cleanup old backups
        cleanup_old_backups
    }
    
    # Cleanup old backups
    cleanup_old_backups() {
        log "Cleaning up old backups (retention: $BACKUP_RETENTION_DAYS days)..."
        
        # This would typically involve cloud storage cleanup
        # For now, just log the action
        log "Old backup cleanup completed"
    }
    
    # Blue-green deployment
    blue_green_deploy() {
        local deployment_name=$1
        local new_image=$2
        
        log "Starting blue-green deployment for $deployment_name with image $new_image"
        
        # Get current deployment
        local current_replicas=$(kubectl get deployment $deployment_name -n $NAMESPACE -o jsonpath='{.spec.replicas}')
        
        # Create green deployment
        local green_deployment="${deployment_name}-green"
        
        # Copy current deployment and modify
        kubectl get deployment $deployment_name -n $NAMESPACE -o yaml | \
        sed "s/name: $deployment_name/name: $green_deployment/g" | \
        sed "s|image: .*|image: $new_image|g" | \
        kubectl apply -f -
        
        # Wait for green deployment to be ready
        kubectl rollout status deployment/$green_deployment -n $NAMESPACE --timeout=300s
        
        # Health check green deployment
        if health_check_deployment $green_deployment; then
            log "Green deployment health check passed"
            
            # Switch traffic to green
            switch_traffic $deployment_name $green_deployment
            
            # Wait and verify
            sleep 30
            if health_check_deployment $green_deployment; then
                log "Traffic switch successful"
                
                # Scale down blue deployment
                kubectl scale deployment $deployment_name -n $NAMESPACE --replicas=0
                
                # Rename deployments
                kubectl delete deployment $deployment_name -n $NAMESPACE
                kubectl patch deployment $green_deployment -n $NAMESPACE -p '{"metadata":{"name":"'$deployment_name'"}}'
                
                log "Blue-green deployment completed successfully"
            else
                error "Health check failed after traffic switch, rolling back"
                rollback_deployment $deployment_name $green_deployment
            fi
        else
            error "Green deployment health check failed, aborting"
            kubectl delete deployment $green_deployment -n $NAMESPACE
            exit 1
        fi
    }
    
    # Health check deployment
    health_check_deployment() {
        local deployment_name=$1
        local max_attempts=10
        local attempt=1
        
        log "Performing health check for deployment $deployment_name"
        
        while [ $attempt -le $max_attempts ]; do
            local ready_replicas=$(kubectl get deployment $deployment_name -n $NAMESPACE -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
            local desired_replicas=$(kubectl get deployment $deployment_name -n $NAMESPACE -o jsonpath='{.spec.replicas}')
            
            if [ "$ready_replicas" = "$desired_replicas" ] && [ "$ready_replicas" != "0" ]; then
                # Additional application-specific health checks
                local pod_name=$(kubectl get pods -n $NAMESPACE -l app=$deployment_name -o jsonpath='{.items[0].metadata.name}')
                
                if kubectl exec $pod_name -n $NAMESPACE -- curl -f http://localhost:8080/health &>/dev/null; then
                    log "Health check passed for $deployment_name"
                    return 0
                fi
            fi
            
            log "Health check attempt $attempt/$max_attempts for $deployment_name"
            sleep 30
            ((attempt++))
        done
        
        error "Health check failed for $deployment_name after $max_attempts attempts"
        return 1
    }
    
    # Switch traffic between deployments
    switch_traffic() {
        local blue_deployment=$1
        local green_deployment=$2
        
        log "Switching traffic from $blue_deployment to $green_deployment"
        
        # Update service selector
        kubectl patch service ${blue_deployment}-service -n $NAMESPACE -p '{"spec":{"selector":{"app":"'$green_deployment'"}}}'
        
        log "Traffic switched to $green_deployment"
    }
    
    # Rollback deployment
    rollback_deployment() {
        local blue_deployment=$1
        local green_deployment=$2
        
        log "Rolling back deployment from $green_deployment to $blue_deployment"
        
        # Switch traffic back to blue
        kubectl patch service ${blue_deployment}-service -n $NAMESPACE -p '{"spec":{"selector":{"app":"'$blue_deployment'"}}}'
        
        # Scale up blue deployment
        kubectl scale deployment $blue_deployment -n $NAMESPACE --replicas=2
        
        # Wait for blue to be ready
        kubectl rollout status deployment/$blue_deployment -n $NAMESPACE --timeout=300s
        
        # Delete green deployment
        kubectl delete deployment $green_deployment -n $NAMESPACE
        
        log "Rollback completed"
    }
    
    # Post-deployment verification
    post_deployment_verification() {
        log "Starting post-deployment verification..."
        
        # Check all deployments are healthy
        local deployments=("nexus-api" "nexus-ai" "nexus-auth" "cache-optimizer" "database-performance-monitor" "monitoring-aggregator")
        
        for deployment in "${deployments[@]}"; do
            if ! health_check_deployment $deployment; then
                error "Post-deployment verification failed for $deployment"
                exit 1
            fi
        done
        
        # Run integration tests
        run_integration_tests
        
        # Check monitoring and alerting
        verify_monitoring
        
        log "Post-deployment verification completed successfully"
    }
    
    # Run integration tests
    run_integration_tests() {
        log "Running integration tests..."
        
        # Create test job
        kubectl apply -f - <<EOF
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: integration-tests-$(date +%s)
      namespace: $NAMESPACE
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: tests
            image: python:3.11-slim
            command:
            - /bin/bash
            - -c
            - |
              pip install requests pytest
              
              # Test API endpoints
              python -c "
              import requests
              import sys
              
              base_url = 'http://nexus-api-service.$NAMESPACE:8080'
              
              # Test health endpoint
              response = requests.get(f'{base_url}/health')
              assert response.status_code == 200, f'Health check failed: {response.status_code}'
              
              # Test authentication endpoint
              response = requests.get(f'{base_url}/api/v1/auth/status')
              assert response.status_code in [200, 401], f'Auth endpoint failed: {response.status_code}'
              
              print('Integration tests passed')
              "
    EOF
        
        # Wait for tests to complete
        local test_job="integration-tests-$(date +%s)"
        kubectl wait --for=condition=complete job/$test_job -n $NAMESPACE --timeout=300s
        
        log "Integration tests completed"
    }
    
    # Verify monitoring
    verify_monitoring() {
        log "Verifying monitoring and alerting..."
        
        # Check Prometheus is scraping metrics
        local prometheus_pod=$(kubectl get pods -n $NAMESPACE -l app=prometheus -o jsonpath='{.items[0].metadata.name}')
        
        if [ -n "$prometheus_pod" ]; then
            # Check if targets are up
            kubectl exec $prometheus_pod -n $NAMESPACE -- wget -qO- http://localhost:9090/api/v1/targets | grep -q '"health":"up"'
            if [ $? -eq 0 ]; then
                log "Prometheus monitoring verified"
            else
                warn "Some Prometheus targets are down"
            fi
        else
            warn "Prometheus pod not found"
        fi
        
        log "Monitoring verification completed"
    }
    
    # Main deployment function
    deploy() {
        local version=${1:-"latest"}
        
        log "Starting Nexus Architect production deployment (version: $version)"
        
        # Pre-deployment checks
        pre_deployment_checks
        
        # Create database backup
        backup_database
        
        # Deploy components using blue-green strategy
        local components=("nexus-api" "nexus-ai" "nexus-auth")
        
        for component in "${components[@]}"; do
            local image="nexus-architect/$component:$version"
            blue_green_deploy $component $image
        done
        
        # Deploy supporting services (direct deployment for stateful services)
        kubectl apply -f /app/manifests/
        
        # Wait for all deployments to be ready
        kubectl rollout status deployment/cache-optimizer -n $NAMESPACE --timeout=300s
        kubectl rollout status deployment/database-performance-monitor -n $NAMESPACE --timeout=300s
        kubectl rollout status deployment/monitoring-aggregator -n $NAMESPACE --timeout=300s
        
        # Post-deployment verification
        post_deployment_verification
        
        log "Nexus Architect production deployment completed successfully"
    }
    
    # Rollback function
    rollback() {
        local target_version=${1:-"previous"}
        
        log "Starting rollback to version: $target_version"
        
        # Implement rollback logic
        # This would typically involve reverting to previous image versions
        # and potentially restoring database backups
        
        log "Rollback completed"
    }
    
    # Main script logic
    case "${1:-deploy}" in
        "deploy")
            deploy "${2:-latest}"
            ;;
        "rollback")
            rollback "${2:-previous}"
            ;;
        "health-check")
            post_deployment_verification
            ;;
        "backup")
            backup_database
            ;;
        *)
            echo "Usage: $0 {deploy|rollback|health-check|backup} [version]"
            exit 1
            ;;
    esac
  
  disaster_recovery.yaml: |
    # Disaster Recovery Plan for Nexus Architect
    
    overview:
      rto: "4 hours"  # Recovery Time Objective
      rpo: "1 hour"   # Recovery Point Objective
      backup_frequency: "every 6 hours"
      backup_retention: "30 days"
      
    backup_strategy:
      database:
        type: "continuous_wal_archiving"
        frequency: "real-time"
        retention: "30 days"
        encryption: "AES-256"
        compression: "gzip"
        
      application_data:
        type: "snapshot"
        frequency: "6 hours"
        retention: "30 days"
        
      configuration:
        type: "git_repository"
        frequency: "on_change"
        retention: "indefinite"
        
      secrets:
        type: "encrypted_backup"
        frequency: "daily"
        retention: "90 days"
    
    recovery_procedures:
      complete_disaster:
        steps:
          1: "Assess damage and determine recovery scope"
          2: "Provision new infrastructure in alternate region"
          3: "Restore database from latest backup"
          4: "Deploy application from container registry"
          5: "Restore configuration and secrets"
          6: "Verify system functionality"
          7: "Update DNS to point to new infrastructure"
          8: "Monitor system stability"
        estimated_time: "4 hours"
        
      database_corruption:
        steps:
          1: "Stop application services"
          2: "Identify last known good backup"
          3: "Restore database from backup"
          4: "Apply WAL logs if available"
          5: "Verify data integrity"
          6: "Restart application services"
          7: "Verify system functionality"
        estimated_time: "2 hours"
        
      application_failure:
        steps:
          1: "Identify failing components"
          2: "Check recent deployments"
          3: "Rollback to previous version if needed"
          4: "Restart failed services"
          5: "Verify system functionality"
        estimated_time: "30 minutes"
    
    testing_schedule:
      disaster_recovery_drill: "quarterly"
      backup_restoration_test: "monthly"
      failover_test: "monthly"
      documentation_review: "quarterly"
    
    contact_information:
      primary_contact: "DevOps Team"
      escalation_contact: "CTO"
      vendor_support: "Cloud Provider Support"
      
    monitoring_alerts:
      backup_failure: "immediate"
      high_rto_risk: "immediate"
      infrastructure_degradation: "15 minutes"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: production-health-monitor
  namespace: nexus-infrastructure
  labels:
    app: production-health-monitor
    component: production-readiness
spec:
  schedule: "*/15 * * * *"  # Every 15 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: production-health-monitor
            component: production-readiness
        spec:
          serviceAccountName: nexus-infrastructure
          restartPolicy: OnFailure
          containers:
          - name: health-monitor
            image: python:3.11-slim
            command:
            - /bin/bash
            - -c
            - |
              # Install required packages
              pip install requests kubernetes pyyaml
              
              # Create production health monitor
              cat > /app/health_monitor.py <<'EOF'
              """
              Production Health Monitor
              Continuous monitoring of production readiness and system health
              """
              
              import os
              import json
              import yaml
              import logging
              import requests
              from datetime import datetime, timedelta
              from typing import Dict, List, Any
              from kubernetes import client, config
              
              # Configure logging
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)
              
              class ProductionHealthMonitor:
                  def __init__(self):
                      # Load Kubernetes config
                      try:
                          config.load_incluster_config()
                      except:
                          config.load_kube_config()
                      
                      self.k8s_apps_v1 = client.AppsV1Api()
                      self.k8s_core_v1 = client.CoreV1Api()
                      self.namespace = "nexus-infrastructure"
                      
                  def check_deployment_health(self) -> Dict[str, Any]:
                      """Check health of all deployments"""
                      try:
                          deployments = self.k8s_apps_v1.list_namespaced_deployment(self.namespace)
                          
                          health_status = {
                              "total_deployments": len(deployments.items),
                              "healthy_deployments": 0,
                              "unhealthy_deployments": [],
                              "deployment_details": []
                          }
                          
                          for deployment in deployments.items:
                              name = deployment.metadata.name
                              replicas = deployment.spec.replicas or 0
                              ready_replicas = deployment.status.ready_replicas or 0
                              
                              is_healthy = ready_replicas == replicas and replicas > 0
                              
                              deployment_info = {
                                  "name": name,
                                  "replicas": replicas,
                                  "ready_replicas": ready_replicas,
                                  "healthy": is_healthy,
                                  "last_updated": deployment.status.conditions[-1].last_update_time.isoformat() if deployment.status.conditions else None
                              }
                              
                              health_status["deployment_details"].append(deployment_info)
                              
                              if is_healthy:
                                  health_status["healthy_deployments"] += 1
                              else:
                                  health_status["unhealthy_deployments"].append(name)
                          
                          return health_status
                      except Exception as e:
                          logger.error(f"Error checking deployment health: {e}")
                          return {"error": str(e)}
                  
                  def check_service_endpoints(self) -> Dict[str, Any]:
                      """Check health of service endpoints"""
                      try:
                          services_to_check = [
                              {"name": "nexus-api-service", "port": 8080, "path": "/health"},
                              {"name": "cache-optimizer-service", "port": 8090, "path": "/health"},
                              {"name": "database-performance-monitor-service", "port": 8091, "path": "/health"},
                              {"name": "monitoring-aggregator-service", "port": 8095, "path": "/health"}
                          ]
                          
                          endpoint_status = {
                              "total_services": len(services_to_check),
                              "healthy_services": 0,
                              "unhealthy_services": [],
                              "service_details": []
                          }
                          
                          for service in services_to_check:
                              try:
                                  url = f"http://{service['name']}.{self.namespace}:{service['port']}{service['path']}"
                                  response = requests.get(url, timeout=10)
                                  
                                  is_healthy = response.status_code == 200
                                  
                                  service_info = {
                                      "name": service["name"],
                                      "url": url,
                                      "status_code": response.status_code,
                                      "healthy": is_healthy,
                                      "response_time": response.elapsed.total_seconds()
                                  }
                                  
                                  endpoint_status["service_details"].append(service_info)
                                  
                                  if is_healthy:
                                      endpoint_status["healthy_services"] += 1
                                  else:
                                      endpoint_status["unhealthy_services"].append(service["name"])
                                      
                              except Exception as e:
                                  logger.error(f"Error checking service {service['name']}: {e}")
                                  endpoint_status["unhealthy_services"].append(service["name"])
                                  endpoint_status["service_details"].append({
                                      "name": service["name"],
                                      "error": str(e),
                                      "healthy": False
                                  })
                          
                          return endpoint_status
                      except Exception as e:
                          logger.error(f"Error checking service endpoints: {e}")
                          return {"error": str(e)}
                  
                  def check_resource_usage(self) -> Dict[str, Any]:
                      """Check resource usage across the cluster"""
                      try:
                          nodes = self.k8s_core_v1.list_node()
                          pods = self.k8s_core_v1.list_namespaced_pod(self.namespace)
                          
                          resource_status = {
                              "node_count": len(nodes.items),
                              "pod_count": len(pods.items),
                              "resource_usage": [],
                              "alerts": []
                          }
                          
                          # Check pod resource usage
                          for pod in pods.items:
                              if pod.status.phase == "Running":
                                  containers = pod.spec.containers
                                  for container in containers:
                                      if container.resources and container.resources.requests:
                                          requests_cpu = container.resources.requests.get("cpu", "0")
                                          requests_memory = container.resources.requests.get("memory", "0")
                                          
                                          resource_status["resource_usage"].append({
                                              "pod": pod.metadata.name,
                                              "container": container.name,
                                              "cpu_request": requests_cpu,
                                              "memory_request": requests_memory
                                          })
                          
                          return resource_status
                      except Exception as e:
                          logger.error(f"Error checking resource usage: {e}")
                          return {"error": str(e)}
                  
                  def check_backup_status(self) -> Dict[str, Any]:
                      """Check backup job status"""
                      try:
                          # Check for recent backup jobs
                          jobs = client.BatchV1Api().list_namespaced_job(self.namespace)
                          
                          backup_status = {
                              "recent_backups": [],
                              "last_successful_backup": None,
                              "backup_health": "unknown"
                          }
                          
                          for job in jobs.items:
                              if "backup" in job.metadata.name:
                                  job_info = {
                                      "name": job.metadata.name,
                                      "creation_time": job.metadata.creation_timestamp.isoformat(),
                                      "completion_time": job.status.completion_time.isoformat() if job.status.completion_time else None,
                                      "succeeded": job.status.succeeded or 0,
                                      "failed": job.status.failed or 0
                                  }
                                  
                                  backup_status["recent_backups"].append(job_info)
                                  
                                  if job.status.succeeded and job.status.succeeded > 0:
                                      if not backup_status["last_successful_backup"] or job.metadata.creation_timestamp > datetime.fromisoformat(backup_status["last_successful_backup"]["creation_time"].replace("Z", "+00:00")):
                                          backup_status["last_successful_backup"] = job_info
                          
                          # Determine backup health
                          if backup_status["last_successful_backup"]:
                              last_backup_time = datetime.fromisoformat(backup_status["last_successful_backup"]["creation_time"].replace("Z", "+00:00"))
                              if datetime.now().replace(tzinfo=last_backup_time.tzinfo) - last_backup_time < timedelta(hours=24):
                                  backup_status["backup_health"] = "healthy"
                              else:
                                  backup_status["backup_health"] = "stale"
                          else:
                              backup_status["backup_health"] = "no_recent_backups"
                          
                          return backup_status
                      except Exception as e:
                          logger.error(f"Error checking backup status: {e}")
                          return {"error": str(e)}
                  
                  def generate_health_report(self) -> Dict[str, Any]:
                      """Generate comprehensive health report"""
                      try:
                          report = {
                              "timestamp": datetime.utcnow().isoformat(),
                              "overall_health": "unknown",
                              "deployment_health": self.check_deployment_health(),
                              "service_health": self.check_service_endpoints(),
                              "resource_status": self.check_resource_usage(),
                              "backup_status": self.check_backup_status(),
                              "alerts": [],
                              "recommendations": []
                          }
                          
                          # Determine overall health
                          health_score = 0
                          total_checks = 0
                          
                          # Deployment health
                          if "error" not in report["deployment_health"]:
                              total_deployments = report["deployment_health"]["total_deployments"]
                              healthy_deployments = report["deployment_health"]["healthy_deployments"]
                              if total_deployments > 0:
                                  health_score += (healthy_deployments / total_deployments) * 25
                              total_checks += 25
                          
                          # Service health
                          if "error" not in report["service_health"]:
                              total_services = report["service_health"]["total_services"]
                              healthy_services = report["service_health"]["healthy_services"]
                              if total_services > 0:
                                  health_score += (healthy_services / total_services) * 25
                              total_checks += 25
                          
                          # Backup health
                          if "error" not in report["backup_status"]:
                              if report["backup_status"]["backup_health"] == "healthy":
                                  health_score += 25
                              elif report["backup_status"]["backup_health"] == "stale":
                                  health_score += 15
                              total_checks += 25
                          
                          # Resource health (simplified)
                          if "error" not in report["resource_status"]:
                              health_score += 25  # Assume healthy if no errors
                              total_checks += 25
                          
                          # Calculate overall health percentage
                          if total_checks > 0:
                              health_percentage = (health_score / total_checks) * 100
                              if health_percentage >= 90:
                                  report["overall_health"] = "healthy"
                              elif health_percentage >= 70:
                                  report["overall_health"] = "degraded"
                              else:
                                  report["overall_health"] = "unhealthy"
                          
                          # Generate alerts and recommendations
                          if report["deployment_health"].get("unhealthy_deployments"):
                              report["alerts"].append(f"Unhealthy deployments: {', '.join(report['deployment_health']['unhealthy_deployments'])}")
                              report["recommendations"].append("Investigate and restart unhealthy deployments")
                          
                          if report["service_health"].get("unhealthy_services"):
                              report["alerts"].append(f"Unhealthy services: {', '.join(report['service_health']['unhealthy_services'])}")
                              report["recommendations"].append("Check service endpoints and network connectivity")
                          
                          if report["backup_status"]["backup_health"] != "healthy":
                              report["alerts"].append(f"Backup status: {report['backup_status']['backup_health']}")
                              report["recommendations"].append("Verify backup jobs and schedule")
                          
                          return report
                      except Exception as e:
                          logger.error(f"Error generating health report: {e}")
                          return {
                              "timestamp": datetime.utcnow().isoformat(),
                              "overall_health": "error",
                              "error": str(e)
                          }
              
              # Main execution
              if __name__ == "__main__":
                  monitor = ProductionHealthMonitor()
                  report = monitor.generate_health_report()
                  
                  # Print report
                  print(json.dumps(report, indent=2))
                  
                  # Log summary
                  logger.info(f"Production health check completed - Status: {report['overall_health']}")
                  
                  if report.get("alerts"):
                      for alert in report["alerts"]:
                          logger.warning(f"ALERT: {alert}")
                  
                  if report.get("recommendations"):
                      for recommendation in report["recommendations"]:
                          logger.info(f"RECOMMENDATION: {recommendation}")
              EOF
              
              # Run the health monitor
              cd /app && python health_monitor.py
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "500m"

