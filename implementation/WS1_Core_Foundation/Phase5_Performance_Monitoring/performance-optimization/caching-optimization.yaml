apiVersion: v1
kind: ConfigMap
metadata:
  name: caching-optimization-config
  namespace: nexus-infrastructure
data:
  redis_optimization.conf: |
    # Redis Performance Optimization Configuration
    # Memory optimization
    maxmemory 8gb
    maxmemory-policy allkeys-lru
    maxmemory-samples 10
    
    # Persistence optimization
    save 900 1
    save 300 10
    save 60 10000
    rdbcompression yes
    rdbchecksum yes
    
    # Network optimization
    tcp-keepalive 300
    tcp-backlog 511
    timeout 0
    
    # Performance tuning
    hash-max-ziplist-entries 512
    hash-max-ziplist-value 64
    list-max-ziplist-size -2
    list-compress-depth 0
    set-max-intset-entries 512
    zset-max-ziplist-entries 128
    zset-max-ziplist-value 64
    
    # Lazy freeing
    lazyfree-lazy-eviction yes
    lazyfree-lazy-expire yes
    lazyfree-lazy-server-del yes
    replica-lazy-flush yes
    
    # Threading
    io-threads 4
    io-threads-do-reads yes
    
  nginx_cache.conf: |
    # Nginx Caching Configuration
    proxy_cache_path /var/cache/nginx/nexus levels=1:2 keys_zone=nexus_cache:100m max_size=10g inactive=60m use_temp_path=off;
    proxy_cache_path /var/cache/nginx/api levels=1:2 keys_zone=api_cache:50m max_size=5g inactive=30m use_temp_path=off;
    proxy_cache_path /var/cache/nginx/static levels=1:2 keys_zone=static_cache:50m max_size=5g inactive=7d use_temp_path=off;
    
    # Cache configuration
    proxy_cache_valid 200 302 10m;
    proxy_cache_valid 404 1m;
    proxy_cache_valid any 5m;
    
    # Cache headers
    proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;
    proxy_cache_background_update on;
    proxy_cache_lock on;
    proxy_cache_lock_timeout 5s;
    proxy_cache_lock_age 5s;
    
    # Bypass cache for certain conditions
    proxy_cache_bypass $http_pragma $http_authorization;
    proxy_no_cache $http_pragma $http_authorization;
    
  application_cache.yaml: |
    # Application-level caching configuration
    cache_layers:
      l1_memory:
        type: "in-memory"
        size: "512MB"
        ttl: "5m"
        eviction_policy: "LRU"
        
      l2_redis:
        type: "redis"
        cluster: "redis.nexus-infrastructure:6379"
        size: "2GB"
        ttl: "1h"
        compression: true
        
      l3_distributed:
        type: "distributed"
        backend: "redis-cluster"
        size: "8GB"
        ttl: "24h"
        replication_factor: 3
        
    cache_strategies:
      api_responses:
        layers: ["l1_memory", "l2_redis"]
        ttl: "10m"
        invalidation: "tag-based"
        
      ai_embeddings:
        layers: ["l2_redis", "l3_distributed"]
        ttl: "7d"
        compression: true
        
      user_sessions:
        layers: ["l2_redis"]
        ttl: "24h"
        sliding_expiration: true
        
      knowledge_base:
        layers: ["l1_memory", "l2_redis", "l3_distributed"]
        ttl: "1d"
        preload: true
        
    cache_warming:
      enabled: true
      strategies:
        - "popular_queries"
        - "user_preferences"
        - "ai_model_responses"
      schedule: "0 2 * * *"  # Daily at 2 AM
      
    cache_monitoring:
      metrics:
        - "hit_ratio"
        - "miss_ratio"
        - "eviction_rate"
        - "memory_usage"
        - "response_time"
      alerts:
        - condition: "hit_ratio < 0.8"
          severity: "warning"
        - condition: "memory_usage > 0.9"
          severity: "critical"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-optimizer
  namespace: nexus-infrastructure
  labels:
    app: cache-optimizer
    component: performance
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cache-optimizer
  template:
    metadata:
      labels:
        app: cache-optimizer
        component: performance
    spec:
      serviceAccountName: nexus-infrastructure
      containers:
      - name: cache-optimizer
        image: python:3.11-slim
        ports:
        - name: http
          containerPort: 8090
        - name: metrics
          containerPort: 9093
        env:
        - name: REDIS_URL
          value: "redis://redis.nexus-infrastructure:6379"
        - name: CACHE_CONFIG_PATH
          value: "/app/config/application_cache.yaml"
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        command:
        - /bin/bash
        - -c
        - |
          # Install required packages
          pip install fastapi uvicorn redis pyyaml prometheus-client \
                     asyncio aioredis cachetools python-json-logger \
                     psutil numpy pandas
          
          # Create cache optimization service
          cat > /app/cache_optimizer.py <<'EOF'
          """
          Cache Optimization Service
          Intelligent caching strategies and performance optimization
          """
          
          import os
          import json
          import yaml
          import logging
          import asyncio
          import time
          import hashlib
          from datetime import datetime, timedelta
          from typing import Dict, Any, Optional, List, Union
          from enum import Enum
          import psutil
          
          from fastapi import FastAPI, HTTPException, BackgroundTasks
          from pydantic import BaseModel, Field
          import redis
          import aioredis
          from cachetools import TTLCache, LRUCache
          from prometheus_client import Counter, Histogram, Gauge, start_http_server
          import numpy as np
          import pandas as pd
          
          # Configure logging
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)
          
          # Prometheus metrics
          cache_hits = Counter('cache_hits_total', 'Cache hits', ['layer', 'cache_type'])
          cache_misses = Counter('cache_misses_total', 'Cache misses', ['layer', 'cache_type'])
          cache_operations = Histogram('cache_operation_duration_seconds', 'Cache operation duration', ['operation', 'layer'])
          cache_memory_usage = Gauge('cache_memory_usage_bytes', 'Cache memory usage', ['layer'])
          cache_evictions = Counter('cache_evictions_total', 'Cache evictions', ['layer', 'reason'])
          
          app = FastAPI(
              title="Cache Optimization Service",
              description="Intelligent caching and performance optimization",
              version="1.0.0"
          )
          
          class CacheLayer(str, Enum):
              L1_MEMORY = "l1_memory"
              L2_REDIS = "l2_redis"
              L3_DISTRIBUTED = "l3_distributed"
          
          class CacheStrategy(str, Enum):
              LRU = "LRU"
              LFU = "LFU"
              TTL = "TTL"
              ADAPTIVE = "adaptive"
          
          class CacheRequest(BaseModel):
              key: str
              value: Any
              ttl: Optional[int] = None
              layers: List[CacheLayer] = [CacheLayer.L1_MEMORY]
              strategy: CacheStrategy = CacheStrategy.LRU
              tags: List[str] = []
          
          class CacheResponse(BaseModel):
              key: str
              value: Any
              hit: bool
              layer: CacheLayer
              ttl_remaining: Optional[int] = None
              metadata: Dict[str, Any] = {}
          
          class CacheOptimizer:
              def __init__(self):
                  self.config = None
                  self.redis_client = None
                  self.l1_cache = None
                  self.cache_stats = {}
                  self.load_configuration()
                  self.initialize_caches()
              
              def load_configuration(self):
                  """Load cache configuration"""
                  try:
                      config_path = os.getenv("CACHE_CONFIG_PATH", "/app/config/application_cache.yaml")
                      with open(config_path, 'r') as f:
                          self.config = yaml.safe_load(f)
                      logger.info("Cache configuration loaded")
                  except Exception as e:
                      logger.error(f"Failed to load configuration: {e}")
                      # Use default configuration
                      self.config = {
                          "cache_layers": {
                              "l1_memory": {"size": "512MB", "ttl": "5m"},
                              "l2_redis": {"size": "2GB", "ttl": "1h"}
                          }
                      }
              
              def initialize_caches(self):
                  """Initialize cache layers"""
                  try:
                      # L1 Memory Cache
                      l1_config = self.config.get("cache_layers", {}).get("l1_memory", {})
                      max_size = self.parse_size(l1_config.get("size", "512MB"))
                      ttl_seconds = self.parse_duration(l1_config.get("ttl", "5m"))
                      
                      self.l1_cache = TTLCache(maxsize=max_size // 1024, ttl=ttl_seconds)  # Approximate
                      
                      # Redis Cache
                      redis_url = os.getenv("REDIS_URL", "redis://redis.nexus-infrastructure:6379")
                      self.redis_client = redis.from_url(redis_url)
                      
                      # Initialize cache statistics
                      self.cache_stats = {
                          "l1_memory": {"hits": 0, "misses": 0, "evictions": 0},
                          "l2_redis": {"hits": 0, "misses": 0, "evictions": 0},
                          "l3_distributed": {"hits": 0, "misses": 0, "evictions": 0}
                      }
                      
                      logger.info("Cache layers initialized")
                  except Exception as e:
                      logger.error(f"Failed to initialize caches: {e}")
                      raise
              
              def parse_size(self, size_str: str) -> int:
                  """Parse size string to bytes"""
                  size_str = size_str.upper()
                  if size_str.endswith('KB'):
                      return int(size_str[:-2]) * 1024
                  elif size_str.endswith('MB'):
                      return int(size_str[:-2]) * 1024 * 1024
                  elif size_str.endswith('GB'):
                      return int(size_str[:-2]) * 1024 * 1024 * 1024
                  else:
                      return int(size_str)
              
              def parse_duration(self, duration_str: str) -> int:
                  """Parse duration string to seconds"""
                  if duration_str.endswith('s'):
                      return int(duration_str[:-1])
                  elif duration_str.endswith('m'):
                      return int(duration_str[:-1]) * 60
                  elif duration_str.endswith('h'):
                      return int(duration_str[:-1]) * 3600
                  elif duration_str.endswith('d'):
                      return int(duration_str[:-1]) * 86400
                  else:
                      return int(duration_str)
              
              async def get(self, key: str, layers: List[CacheLayer] = None) -> Optional[CacheResponse]:
                  """Get value from cache with multi-layer lookup"""
                  if layers is None:
                      layers = [CacheLayer.L1_MEMORY, CacheLayer.L2_REDIS]
                  
                  for layer in layers:
                      try:
                          with cache_operations.labels(operation="get", layer=layer.value).time():
                              value = await self._get_from_layer(key, layer)
                              if value is not None:
                                  cache_hits.labels(layer=layer.value, cache_type="get").inc()
                                  self.cache_stats[layer.value]["hits"] += 1
                                  
                                  # Promote to higher layers
                                  await self._promote_to_higher_layers(key, value, layer, layers)
                                  
                                  return CacheResponse(
                                      key=key,
                                      value=value,
                                      hit=True,
                                      layer=layer,
                                      metadata={"source_layer": layer.value}
                                  )
                              else:
                                  cache_misses.labels(layer=layer.value, cache_type="get").inc()
                                  self.cache_stats[layer.value]["misses"] += 1
                      except Exception as e:
                          logger.error(f"Error getting from {layer.value}: {e}")
                          continue
                  
                  return None
              
              async def _get_from_layer(self, key: str, layer: CacheLayer) -> Optional[Any]:
                  """Get value from specific cache layer"""
                  if layer == CacheLayer.L1_MEMORY:
                      return self.l1_cache.get(key)
                  elif layer == CacheLayer.L2_REDIS:
                      try:
                          value = self.redis_client.get(f"nexus:cache:{key}")
                          if value:
                              return json.loads(value)
                      except Exception as e:
                          logger.error(f"Redis get error: {e}")
                  elif layer == CacheLayer.L3_DISTRIBUTED:
                      # Placeholder for distributed cache implementation
                      pass
                  
                  return None
              
              async def _promote_to_higher_layers(self, key: str, value: Any, source_layer: CacheLayer, layers: List[CacheLayer]):
                  """Promote cache entry to higher layers"""
                  source_index = layers.index(source_layer)
                  for i in range(source_index):
                      higher_layer = layers[i]
                      try:
                          await self._set_to_layer(key, value, higher_layer)
                      except Exception as e:
                          logger.error(f"Error promoting to {higher_layer.value}: {e}")
              
              async def set(self, request: CacheRequest) -> bool:
                  """Set value in cache layers"""
                  success = True
                  
                  for layer in request.layers:
                      try:
                          with cache_operations.labels(operation="set", layer=layer.value).time():
                              await self._set_to_layer(request.key, request.value, layer, request.ttl)
                      except Exception as e:
                          logger.error(f"Error setting to {layer.value}: {e}")
                          success = False
                  
                  return success
              
              async def _set_to_layer(self, key: str, value: Any, layer: CacheLayer, ttl: Optional[int] = None):
                  """Set value to specific cache layer"""
                  if layer == CacheLayer.L1_MEMORY:
                      self.l1_cache[key] = value
                  elif layer == CacheLayer.L2_REDIS:
                      try:
                          serialized_value = json.dumps(value)
                          if ttl:
                              self.redis_client.setex(f"nexus:cache:{key}", ttl, serialized_value)
                          else:
                              self.redis_client.set(f"nexus:cache:{key}", serialized_value)
                      except Exception as e:
                          logger.error(f"Redis set error: {e}")
                          raise
                  elif layer == CacheLayer.L3_DISTRIBUTED:
                      # Placeholder for distributed cache implementation
                      pass
              
              async def delete(self, key: str, layers: List[CacheLayer] = None) -> bool:
                  """Delete value from cache layers"""
                  if layers is None:
                      layers = [CacheLayer.L1_MEMORY, CacheLayer.L2_REDIS]
                  
                  success = True
                  for layer in layers:
                      try:
                          with cache_operations.labels(operation="delete", layer=layer.value).time():
                              await self._delete_from_layer(key, layer)
                      except Exception as e:
                          logger.error(f"Error deleting from {layer.value}: {e}")
                          success = False
                  
                  return success
              
              async def _delete_from_layer(self, key: str, layer: CacheLayer):
                  """Delete value from specific cache layer"""
                  if layer == CacheLayer.L1_MEMORY:
                      self.l1_cache.pop(key, None)
                  elif layer == CacheLayer.L2_REDIS:
                      try:
                          self.redis_client.delete(f"nexus:cache:{key}")
                      except Exception as e:
                          logger.error(f"Redis delete error: {e}")
                          raise
                  elif layer == CacheLayer.L3_DISTRIBUTED:
                      # Placeholder for distributed cache implementation
                      pass
              
              async def invalidate_by_tags(self, tags: List[str]) -> int:
                  """Invalidate cache entries by tags"""
                  invalidated = 0
                  
                  try:
                      # Get all keys with tags from Redis
                      for tag in tags:
                          pattern = f"nexus:cache:tag:{tag}:*"
                          keys = self.redis_client.keys(pattern)
                          
                          for key in keys:
                              # Extract original key from tag key
                              original_key = key.decode().split(':')[-1]
                              await self.delete(original_key)
                              invalidated += 1
                  except Exception as e:
                      logger.error(f"Error invalidating by tags: {e}")
                  
                  return invalidated
              
              def get_cache_stats(self) -> Dict[str, Any]:
                  """Get cache statistics"""
                  stats = {}
                  
                  # L1 Memory Cache stats
                  l1_info = self.l1_cache.currsize if hasattr(self.l1_cache, 'currsize') else 0
                  stats["l1_memory"] = {
                      "size": l1_info,
                      "max_size": self.l1_cache.maxsize if hasattr(self.l1_cache, 'maxsize') else 0,
                      "hits": self.cache_stats["l1_memory"]["hits"],
                      "misses": self.cache_stats["l1_memory"]["misses"],
                      "hit_ratio": self._calculate_hit_ratio("l1_memory")
                  }
                  
                  # Redis Cache stats
                  try:
                      redis_info = self.redis_client.info('memory')
                      stats["l2_redis"] = {
                          "memory_used": redis_info.get('used_memory', 0),
                          "memory_peak": redis_info.get('used_memory_peak', 0),
                          "hits": self.cache_stats["l2_redis"]["hits"],
                          "misses": self.cache_stats["l2_redis"]["misses"],
                          "hit_ratio": self._calculate_hit_ratio("l2_redis")
                      }
                  except Exception as e:
                      logger.error(f"Error getting Redis stats: {e}")
                      stats["l2_redis"] = {"error": str(e)}
                  
                  return stats
              
              def _calculate_hit_ratio(self, layer: str) -> float:
                  """Calculate hit ratio for cache layer"""
                  hits = self.cache_stats[layer]["hits"]
                  misses = self.cache_stats[layer]["misses"]
                  total = hits + misses
                  return hits / total if total > 0 else 0.0
              
              async def optimize_cache(self):
                  """Perform cache optimization"""
                  try:
                      # Analyze cache performance
                      stats = self.get_cache_stats()
                      
                      # Optimize based on hit ratios
                      for layer, layer_stats in stats.items():
                          if isinstance(layer_stats, dict) and "hit_ratio" in layer_stats:
                              hit_ratio = layer_stats["hit_ratio"]
                              
                              if hit_ratio < 0.7:  # Low hit ratio
                                  logger.warning(f"Low hit ratio for {layer}: {hit_ratio:.2f}")
                                  await self._optimize_layer(layer)
                      
                      # Update metrics
                      for layer, layer_stats in stats.items():
                          if isinstance(layer_stats, dict):
                              if "memory_used" in layer_stats:
                                  cache_memory_usage.labels(layer=layer).set(layer_stats["memory_used"])
                              elif "size" in layer_stats:
                                  cache_memory_usage.labels(layer=layer).set(layer_stats["size"])
                      
                  except Exception as e:
                      logger.error(f"Cache optimization error: {e}")
              
              async def _optimize_layer(self, layer: str):
                  """Optimize specific cache layer"""
                  if layer == "l1_memory":
                      # Clear least recently used items
                      if hasattr(self.l1_cache, 'clear'):
                          # Partial clear to make room
                          current_size = getattr(self.l1_cache, 'currsize', 0)
                          max_size = getattr(self.l1_cache, 'maxsize', 1000)
                          if current_size > max_size * 0.8:
                              # Clear 20% of cache
                              items_to_remove = int(current_size * 0.2)
                              for _ in range(items_to_remove):
                                  try:
                                      self.l1_cache.popitem()
                                  except KeyError:
                                      break
                  elif layer == "l2_redis":
                      # Redis optimization handled by Redis configuration
                      pass
          
          # Initialize optimizer
          optimizer = CacheOptimizer()
          
          # API Endpoints
          @app.post("/api/v1/cache/set")
          async def set_cache(request: CacheRequest):
              """Set value in cache"""
              success = await optimizer.set(request)
              return {"success": success, "key": request.key}
          
          @app.get("/api/v1/cache/get/{key}")
          async def get_cache(key: str, layers: str = "l1_memory,l2_redis"):
              """Get value from cache"""
              layer_list = [CacheLayer(layer.strip()) for layer in layers.split(",")]
              result = await optimizer.get(key, layer_list)
              
              if result:
                  return result.dict()
              else:
                  raise HTTPException(status_code=404, detail="Key not found in cache")
          
          @app.delete("/api/v1/cache/delete/{key}")
          async def delete_cache(key: str, layers: str = "l1_memory,l2_redis"):
              """Delete value from cache"""
              layer_list = [CacheLayer(layer.strip()) for layer in layers.split(",")]
              success = await optimizer.delete(key, layer_list)
              return {"success": success, "key": key}
          
          @app.post("/api/v1/cache/invalidate")
          async def invalidate_cache(tags: List[str]):
              """Invalidate cache by tags"""
              count = await optimizer.invalidate_by_tags(tags)
              return {"invalidated_count": count, "tags": tags}
          
          @app.get("/api/v1/cache/stats")
          async def get_cache_statistics():
              """Get cache statistics"""
              return optimizer.get_cache_stats()
          
          @app.post("/api/v1/cache/optimize")
          async def optimize_cache():
              """Trigger cache optimization"""
              await optimizer.optimize_cache()
              return {"status": "optimization_completed"}
          
          @app.get("/health")
          async def health_check():
              return {"status": "healthy", "service": "cache-optimizer"}
          
          @app.get("/metrics")
          async def get_metrics():
              """Prometheus metrics endpoint"""
              from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
              return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
          
          # Background tasks
          @app.on_event("startup")
          async def startup_event():
              # Start Prometheus metrics server
              start_http_server(9093)
              
              # Start background optimization task
              asyncio.create_task(background_optimization())
              
              logger.info("Cache optimization service started")
          
          async def background_optimization():
              """Background cache optimization task"""
              while True:
                  try:
                      await asyncio.sleep(300)  # Run every 5 minutes
                      await optimizer.optimize_cache()
                  except Exception as e:
                      logger.error(f"Background optimization error: {e}")
          
          if __name__ == "__main__":
              import uvicorn
              uvicorn.run(app, host="0.0.0.0", port=8090)
          EOF
          
          # Start the service
          cd /app && python cache_optimizer.py
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8090
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8090
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      volumes:
      - name: config-volume
        configMap:
          name: caching-optimization-config
---
apiVersion: v1
kind: Service
metadata:
  name: cache-optimizer-service
  namespace: nexus-infrastructure
  labels:
    app: cache-optimizer
    component: performance
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 8090
    targetPort: 8090
    protocol: TCP
  - name: metrics
    port: 9093
    targetPort: 9093
    protocol: TCP
  selector:
    app: cache-optimizer

