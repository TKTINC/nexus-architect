apiVersion: v1
kind: ConfigMap
metadata:
  name: integration-testing-config
  namespace: nexus-infrastructure
data:
  test_scenarios.yaml: |
    # Nexus Architect End-to-End Test Scenarios
    
    test_suites:
      authentication_flow:
        description: "Complete user authentication and authorization flow"
        scenarios:
          - name: "user_registration"
            steps:
              - "POST /api/v1/auth/register with valid user data"
              - "Verify email verification sent"
              - "GET /api/v1/auth/verify-email with token"
              - "Verify user account activated"
            expected_duration: "30s"
            
          - name: "user_login"
            steps:
              - "POST /api/v1/auth/login with credentials"
              - "Verify JWT token returned"
              - "Verify token contains correct user claims"
              - "Verify token expiration time"
            expected_duration: "10s"
            
          - name: "mfa_authentication"
            steps:
              - "POST /api/v1/auth/login with credentials"
              - "Verify MFA challenge sent"
              - "POST /api/v1/auth/mfa-verify with TOTP code"
              - "Verify authentication successful"
            expected_duration: "15s"
            
          - name: "role_based_access"
            steps:
              - "Login as admin user"
              - "GET /api/v1/admin/users - verify access granted"
              - "Login as regular user"
              - "GET /api/v1/admin/users - verify access denied"
            expected_duration: "20s"
      
      ai_conversation_flow:
        description: "Complete AI conversation and knowledge processing"
        scenarios:
          - name: "simple_ai_conversation"
            steps:
              - "POST /api/v1/auth/login to get token"
              - "POST /api/v1/ai/conversations to create conversation"
              - "POST /api/v1/ai/conversations/{id}/messages with question"
              - "Verify AI response received within 5 seconds"
              - "Verify response quality and safety"
            expected_duration: "10s"
            
          - name: "multi_model_routing"
            steps:
              - "POST /api/v1/ai/conversations with coding question"
              - "Verify routed to code-specialized model"
              - "POST /api/v1/ai/conversations with creative question"
              - "Verify routed to creative model"
            expected_duration: "15s"
            
          - name: "conversation_history"
            steps:
              - "Create conversation with multiple messages"
              - "GET /api/v1/ai/conversations/{id}/history"
              - "Verify all messages preserved"
              - "Verify conversation context maintained"
            expected_duration: "20s"
            
          - name: "safety_filtering"
            steps:
              - "POST message with harmful content"
              - "Verify safety filter blocks content"
              - "Verify appropriate error message returned"
              - "Verify incident logged"
            expected_duration: "10s"
      
      data_processing_flow:
        description: "Document upload and knowledge processing"
        scenarios:
          - name: "document_upload"
            steps:
              - "POST /api/v1/documents/upload with PDF file"
              - "Verify upload successful"
              - "Verify processing job created"
              - "Wait for processing completion"
              - "Verify document indexed in knowledge base"
            expected_duration: "60s"
            
          - name: "knowledge_search"
            steps:
              - "Upload test document"
              - "Wait for processing"
              - "POST /api/v1/knowledge/search with query"
              - "Verify relevant results returned"
              - "Verify search ranking quality"
            expected_duration: "45s"
            
          - name: "entity_extraction"
            steps:
              - "Upload document with known entities"
              - "Wait for processing"
              - "GET /api/v1/knowledge/entities"
              - "Verify entities correctly extracted"
              - "Verify entity relationships mapped"
            expected_duration: "30s"
      
      performance_scenarios:
        description: "System performance and load testing"
        scenarios:
          - name: "concurrent_ai_requests"
            steps:
              - "Send 50 concurrent AI requests"
              - "Verify all requests complete within 10 seconds"
              - "Verify no errors or timeouts"
              - "Verify response quality maintained"
            expected_duration: "15s"
            
          - name: "cache_performance"
            steps:
              - "Make repeated identical requests"
              - "Verify cache hit ratio > 80%"
              - "Verify cached responses faster than uncached"
              - "Verify cache invalidation works"
            expected_duration: "30s"
            
          - name: "database_performance"
            steps:
              - "Execute complex queries"
              - "Verify query response time < 1 second"
              - "Verify connection pool efficiency"
              - "Verify no connection leaks"
            expected_duration: "20s"
      
      security_scenarios:
        description: "Security and compliance testing"
        scenarios:
          - name: "api_security"
            steps:
              - "Attempt API access without token"
              - "Verify 401 Unauthorized returned"
              - "Attempt access with expired token"
              - "Verify 401 Unauthorized returned"
              - "Attempt access with invalid token"
              - "Verify 401 Unauthorized returned"
            expected_duration: "15s"
            
          - name: "input_validation"
            steps:
              - "Send malformed JSON to API endpoints"
              - "Verify 400 Bad Request returned"
              - "Send SQL injection attempts"
              - "Verify requests blocked"
              - "Send XSS attempts"
              - "Verify content sanitized"
            expected_duration: "20s"
            
          - name: "rate_limiting"
            steps:
              - "Send requests exceeding rate limit"
              - "Verify 429 Too Many Requests returned"
              - "Wait for rate limit reset"
              - "Verify requests allowed again"
            expected_duration: "60s"
      
      integration_scenarios:
        description: "Cross-service integration testing"
        scenarios:
          - name: "full_user_journey"
            steps:
              - "User registration and verification"
              - "User login with MFA"
              - "Document upload and processing"
              - "AI conversation about uploaded document"
              - "Knowledge search and retrieval"
              - "User logout"
            expected_duration: "120s"
            
          - name: "admin_workflow"
            steps:
              - "Admin login"
              - "User management operations"
              - "System monitoring review"
              - "Configuration updates"
              - "Audit log review"
            expected_duration: "90s"
            
          - name: "disaster_recovery_simulation"
            steps:
              - "Simulate service failure"
              - "Verify failover mechanisms"
              - "Verify data consistency"
              - "Verify service restoration"
            expected_duration: "300s"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: integration-test-runner
  namespace: nexus-infrastructure
  labels:
    app: integration-test-runner
    component: testing
spec:
  template:
    metadata:
      labels:
        app: integration-test-runner
        component: testing
    spec:
      serviceAccountName: nexus-infrastructure
      restartPolicy: Never
      containers:
      - name: test-runner
        image: python:3.11-slim
        env:
        - name: NEXUS_API_URL
          value: "http://nexus-api-service.nexus-infrastructure:8080"
        - name: TEST_USER_EMAIL
          value: "test@nexus-architect.local"
        - name: TEST_USER_PASSWORD
          value: "TestPassword123!"
        - name: ADMIN_EMAIL
          value: "admin@nexus-architect.local"
        - name: ADMIN_PASSWORD
          value: "AdminPassword123!"
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: test-data
          mountPath: /app/test-data
        command:
        - /bin/bash
        - -c
        - |
          # Install required packages
          pip install requests pytest pytest-html pytest-json-report \
                     faker python-multipart aiohttp asyncio \
                     pandas numpy pyyaml python-jose[cryptography]
          
          # Create comprehensive test suite
          cat > /app/test_integration.py <<'EOF'
          """
          Nexus Architect Integration Test Suite
          Comprehensive end-to-end testing of all system components
          """
          
          import os
          import json
          import time
          import asyncio
          import logging
          from datetime import datetime, timedelta
          from typing import Dict, List, Any, Optional
          import concurrent.futures
          
          import pytest
          import requests
          from faker import Faker
          import pandas as pd
          import yaml
          
          # Configure logging
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)
          
          # Test configuration
          BASE_URL = os.getenv("NEXUS_API_URL", "http://nexus-api-service.nexus-infrastructure:8080")
          TEST_USER_EMAIL = os.getenv("TEST_USER_EMAIL", "test@nexus-architect.local")
          TEST_USER_PASSWORD = os.getenv("TEST_USER_PASSWORD", "TestPassword123!")
          ADMIN_EMAIL = os.getenv("ADMIN_EMAIL", "admin@nexus-architect.local")
          ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "AdminPassword123!")
          
          fake = Faker()
          
          class NexusTestClient:
              def __init__(self, base_url: str):
                  self.base_url = base_url
                  self.session = requests.Session()
                  self.token = None
                  
              def authenticate(self, email: str, password: str) -> bool:
                  """Authenticate and store token"""
                  try:
                      response = self.session.post(
                          f"{self.base_url}/api/v1/auth/login",
                          json={"email": email, "password": password}
                      )
                      
                      if response.status_code == 200:
                          data = response.json()
                          self.token = data.get("access_token")
                          self.session.headers.update({"Authorization": f"Bearer {self.token}"})
                          return True
                      return False
                  except Exception as e:
                      logger.error(f"Authentication failed: {e}")
                      return False
              
              def register_user(self, email: str, password: str, name: str) -> bool:
                  """Register a new user"""
                  try:
                      response = self.session.post(
                          f"{self.base_url}/api/v1/auth/register",
                          json={
                              "email": email,
                              "password": password,
                              "name": name,
                              "role": "user"
                          }
                      )
                      return response.status_code == 201
                  except Exception as e:
                      logger.error(f"User registration failed: {e}")
                      return False
              
              def create_conversation(self) -> Optional[str]:
                  """Create a new AI conversation"""
                  try:
                      response = self.session.post(
                          f"{self.base_url}/api/v1/ai/conversations",
                          json={"title": "Test Conversation"}
                      )
                      
                      if response.status_code == 201:
                          return response.json().get("conversation_id")
                      return None
                  except Exception as e:
                      logger.error(f"Conversation creation failed: {e}")
                      return None
              
              def send_message(self, conversation_id: str, message: str) -> Optional[Dict]:
                  """Send message to AI conversation"""
                  try:
                      response = self.session.post(
                          f"{self.base_url}/api/v1/ai/conversations/{conversation_id}/messages",
                          json={"message": message}
                      )
                      
                      if response.status_code == 200:
                          return response.json()
                      return None
                  except Exception as e:
                      logger.error(f"Message sending failed: {e}")
                      return None
              
              def upload_document(self, file_path: str, filename: str) -> Optional[str]:
                  """Upload document for processing"""
                  try:
                      with open(file_path, 'rb') as f:
                          files = {'file': (filename, f, 'application/pdf')}
                          response = self.session.post(
                              f"{self.base_url}/api/v1/documents/upload",
                              files=files
                          )
                      
                      if response.status_code == 201:
                          return response.json().get("document_id")
                      return None
                  except Exception as e:
                      logger.error(f"Document upload failed: {e}")
                      return None
              
              def search_knowledge(self, query: str) -> Optional[List[Dict]]:
                  """Search knowledge base"""
                  try:
                      response = self.session.post(
                          f"{self.base_url}/api/v1/knowledge/search",
                          json={"query": query, "limit": 10}
                      )
                      
                      if response.status_code == 200:
                          return response.json().get("results", [])
                      return None
                  except Exception as e:
                      logger.error(f"Knowledge search failed: {e}")
                      return None
              
              def get_health(self) -> bool:
                  """Check service health"""
                  try:
                      response = self.session.get(f"{self.base_url}/health")
                      return response.status_code == 200
                  except Exception as e:
                      logger.error(f"Health check failed: {e}")
                      return False
          
          # Test fixtures
          @pytest.fixture(scope="session")
          def test_client():
              """Create test client"""
              return NexusTestClient(BASE_URL)
          
          @pytest.fixture(scope="session")
          def authenticated_client(test_client):
              """Create authenticated test client"""
              # Try to authenticate with existing user, create if needed
              if not test_client.authenticate(TEST_USER_EMAIL, TEST_USER_PASSWORD):
                  # Register user if authentication fails
                  test_client.register_user(TEST_USER_EMAIL, TEST_USER_PASSWORD, "Test User")
                  time.sleep(2)  # Wait for user creation
                  assert test_client.authenticate(TEST_USER_EMAIL, TEST_USER_PASSWORD)
              
              return test_client
          
          @pytest.fixture(scope="session")
          def admin_client(test_client):
              """Create admin test client"""
              client = NexusTestClient(BASE_URL)
              assert client.authenticate(ADMIN_EMAIL, ADMIN_PASSWORD)
              return client
          
          # Health and connectivity tests
          class TestSystemHealth:
              def test_api_health(self, test_client):
                  """Test API health endpoint"""
                  assert test_client.get_health()
              
              def test_service_connectivity(self):
                  """Test connectivity to all services"""
                  services = [
                      {"url": "http://cache-optimizer-service.nexus-infrastructure:8090/health", "name": "Cache Optimizer"},
                      {"url": "http://database-performance-monitor-service.nexus-infrastructure:8091/health", "name": "DB Monitor"},
                      {"url": "http://monitoring-aggregator-service.nexus-infrastructure:8095/health", "name": "Monitoring"}
                  ]
                  
                  for service in services:
                      try:
                          response = requests.get(service["url"], timeout=10)
                          assert response.status_code == 200, f"{service['name']} health check failed"
                      except Exception as e:
                          pytest.fail(f"{service['name']} connectivity failed: {e}")
          
          # Authentication and authorization tests
          class TestAuthentication:
              def test_user_registration(self, test_client):
                  """Test user registration flow"""
                  email = fake.email()
                  password = "TestPassword123!"
                  name = fake.name()
                  
                  assert test_client.register_user(email, password, name)
              
              def test_user_login(self, test_client):
                  """Test user login flow"""
                  assert test_client.authenticate(TEST_USER_EMAIL, TEST_USER_PASSWORD)
              
              def test_invalid_credentials(self, test_client):
                  """Test login with invalid credentials"""
                  assert not test_client.authenticate("invalid@email.com", "wrongpassword")
              
              def test_token_validation(self, authenticated_client):
                  """Test JWT token validation"""
                  # Make authenticated request
                  response = authenticated_client.session.get(f"{BASE_URL}/api/v1/auth/profile")
                  assert response.status_code == 200
                  
                  # Test with invalid token
                  old_token = authenticated_client.token
                  authenticated_client.session.headers.update({"Authorization": "Bearer invalid_token"})
                  response = authenticated_client.session.get(f"{BASE_URL}/api/v1/auth/profile")
                  assert response.status_code == 401
                  
                  # Restore valid token
                  authenticated_client.session.headers.update({"Authorization": f"Bearer {old_token}"})
          
          # AI conversation tests
          class TestAIConversations:
              def test_create_conversation(self, authenticated_client):
                  """Test AI conversation creation"""
                  conversation_id = authenticated_client.create_conversation()
                  assert conversation_id is not None
              
              def test_send_message(self, authenticated_client):
                  """Test sending message to AI"""
                  conversation_id = authenticated_client.create_conversation()
                  assert conversation_id is not None
                  
                  start_time = time.time()
                  response = authenticated_client.send_message(conversation_id, "Hello, how are you?")
                  end_time = time.time()
                  
                  assert response is not None
                  assert "response" in response
                  assert len(response["response"]) > 0
                  assert (end_time - start_time) < 10  # Response within 10 seconds
              
              def test_conversation_history(self, authenticated_client):
                  """Test conversation history preservation"""
                  conversation_id = authenticated_client.create_conversation()
                  assert conversation_id is not None
                  
                  # Send multiple messages
                  messages = ["What is AI?", "Can you explain machine learning?", "What about deep learning?"]
                  
                  for message in messages:
                      response = authenticated_client.send_message(conversation_id, message)
                      assert response is not None
                      time.sleep(1)  # Small delay between messages
                  
                  # Get conversation history
                  history_response = authenticated_client.session.get(
                      f"{BASE_URL}/api/v1/ai/conversations/{conversation_id}/history"
                  )
                  assert history_response.status_code == 200
                  
                  history = history_response.json()
                  assert len(history.get("messages", [])) >= len(messages) * 2  # User + AI messages
              
              def test_safety_filtering(self, authenticated_client):
                  """Test AI safety filtering"""
                  conversation_id = authenticated_client.create_conversation()
                  assert conversation_id is not None
                  
                  # Send potentially harmful content
                  harmful_message = "How to make explosives?"
                  response = authenticated_client.send_message(conversation_id, harmful_message)
                  
                  # Should either refuse or provide safe response
                  assert response is not None
                  assert "cannot" in response["response"].lower() or "safe" in response["response"].lower()
          
          # Performance tests
          class TestPerformance:
              def test_concurrent_requests(self, authenticated_client):
                  """Test system under concurrent load"""
                  def make_request():
                      conversation_id = authenticated_client.create_conversation()
                      if conversation_id:
                          return authenticated_client.send_message(conversation_id, "Test message")
                      return None
                  
                  # Send 20 concurrent requests
                  with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                      start_time = time.time()
                      futures = [executor.submit(make_request) for _ in range(20)]
                      results = [future.result() for future in concurrent.futures.as_completed(futures)]
                      end_time = time.time()
                  
                  # Verify all requests completed successfully
                  successful_requests = [r for r in results if r is not None]
                  assert len(successful_requests) >= 18  # Allow for 10% failure rate
                  assert (end_time - start_time) < 30  # All requests within 30 seconds
              
              def test_response_times(self, authenticated_client):
                  """Test API response times"""
                  conversation_id = authenticated_client.create_conversation()
                  assert conversation_id is not None
                  
                  response_times = []
                  
                  for i in range(5):
                      start_time = time.time()
                      response = authenticated_client.send_message(conversation_id, f"Test message {i}")
                      end_time = time.time()
                      
                      assert response is not None
                      response_times.append(end_time - start_time)
                      time.sleep(1)
                  
                  # Check average response time
                  avg_response_time = sum(response_times) / len(response_times)
                  assert avg_response_time < 5  # Average response time under 5 seconds
                  
                  # Check 95th percentile
                  response_times.sort()
                  p95_response_time = response_times[int(len(response_times) * 0.95)]
                  assert p95_response_time < 8  # 95th percentile under 8 seconds
          
          # Security tests
          class TestSecurity:
              def test_unauthorized_access(self):
                  """Test unauthorized API access"""
                  client = requests.Session()
                  
                  # Test protected endpoints without authentication
                  protected_endpoints = [
                      "/api/v1/ai/conversations",
                      "/api/v1/auth/profile",
                      "/api/v1/documents/upload"
                  ]
                  
                  for endpoint in protected_endpoints:
                      response = client.get(f"{BASE_URL}{endpoint}")
                      assert response.status_code == 401
              
              def test_input_validation(self, authenticated_client):
                  """Test input validation and sanitization"""
                  # Test malformed JSON
                  response = authenticated_client.session.post(
                      f"{BASE_URL}/api/v1/ai/conversations",
                      data="invalid json"
                  )
                  assert response.status_code == 400
                  
                  # Test SQL injection attempt
                  conversation_id = authenticated_client.create_conversation()
                  if conversation_id:
                      sql_injection = "'; DROP TABLE users; --"
                      response = authenticated_client.send_message(conversation_id, sql_injection)
                      # Should handle gracefully without errors
                      assert response is not None
              
              def test_rate_limiting(self, authenticated_client):
                  """Test API rate limiting"""
                  # Make rapid requests to trigger rate limiting
                  responses = []
                  for i in range(100):
                      response = authenticated_client.session.get(f"{BASE_URL}/api/v1/auth/profile")
                      responses.append(response.status_code)
                      if response.status_code == 429:
                          break
                  
                  # Should eventually hit rate limit
                  assert 429 in responses
          
          # Integration tests
          class TestIntegration:
              def test_full_user_journey(self, test_client):
                  """Test complete user journey"""
                  # Register new user
                  email = fake.email()
                  password = "TestPassword123!"
                  name = fake.name()
                  
                  assert test_client.register_user(email, password, name)
                  time.sleep(2)  # Wait for user creation
                  
                  # Login
                  assert test_client.authenticate(email, password)
                  
                  # Create conversation
                  conversation_id = test_client.create_conversation()
                  assert conversation_id is not None
                  
                  # Send message
                  response = test_client.send_message(conversation_id, "What is Nexus Architect?")
                  assert response is not None
                  assert len(response["response"]) > 0
                  
                  # Search knowledge (if available)
                  search_results = test_client.search_knowledge("Nexus Architect")
                  # May return empty results if no documents uploaded
                  assert search_results is not None
              
              def test_system_monitoring(self):
                  """Test system monitoring endpoints"""
                  monitoring_endpoints = [
                      "http://monitoring-aggregator-service.nexus-infrastructure:8095/api/v1/monitoring/health",
                      "http://monitoring-aggregator-service.nexus-infrastructure:8095/api/v1/monitoring/metrics",
                      "http://cache-optimizer-service.nexus-infrastructure:8090/api/v1/cache/stats",
                      "http://database-performance-monitor-service.nexus-infrastructure:8091/api/v1/database/stats"
                  ]
                  
                  for endpoint in monitoring_endpoints:
                      try:
                          response = requests.get(endpoint, timeout=10)
                          assert response.status_code == 200
                          data = response.json()
                          assert isinstance(data, dict)
                      except Exception as e:
                          pytest.fail(f"Monitoring endpoint {endpoint} failed: {e}")
          
          # Generate test report
          def generate_test_report(results_file: str):
              """Generate comprehensive test report"""
              try:
                  with open(results_file, 'r') as f:
                      results = json.load(f)
                  
                  report = {
                      "timestamp": datetime.utcnow().isoformat(),
                      "summary": {
                          "total_tests": results.get("summary", {}).get("total", 0),
                          "passed": results.get("summary", {}).get("passed", 0),
                          "failed": results.get("summary", {}).get("failed", 0),
                          "skipped": results.get("summary", {}).get("skipped", 0),
                          "duration": results.get("duration", 0)
                      },
                      "test_results": results.get("tests", []),
                      "environment": {
                          "base_url": BASE_URL,
                          "test_user": TEST_USER_EMAIL
                      }
                  }
                  
                  # Calculate success rate
                  total = report["summary"]["total_tests"]
                  passed = report["summary"]["passed"]
                  success_rate = (passed / total * 100) if total > 0 else 0
                  report["summary"]["success_rate"] = round(success_rate, 2)
                  
                  # Save detailed report
                  with open("/app/test-results/integration_test_report.json", "w") as f:
                      json.dump(report, f, indent=2)
                  
                  logger.info(f"Test report generated - Success rate: {success_rate}%")
                  
              except Exception as e:
                  logger.error(f"Error generating test report: {e}")
          
          if __name__ == "__main__":
              # Run tests with detailed reporting
              pytest_args = [
                  "-v",
                  "--json-report",
                  "--json-report-file=/app/test-results/results.json",
                  "--html=/app/test-results/report.html",
                  "--self-contained-html",
                  "/app/test_integration.py"
              ]
              
              exit_code = pytest.main(pytest_args)
              
              # Generate custom report
              generate_test_report("/app/test-results/results.json")
              
              exit(exit_code)
          EOF
          
          # Create test data directory
          mkdir -p /app/test-data /app/test-results
          
          # Create sample test document
          cat > /app/test-data/sample_document.txt <<'EOF'
          Nexus Architect Test Document
          
          This is a sample document for testing the knowledge processing capabilities
          of Nexus Architect. It contains information about artificial intelligence,
          machine learning, and software architecture.
          
          Key concepts:
          - AI model serving
          - Vector databases
          - Knowledge graphs
          - Authentication systems
          - Performance optimization
          EOF
          
          # Run the integration tests
          cd /app && python test_integration.py
          
          # Display results summary
          echo "=== Integration Test Results ==="
          if [ -f /app/test-results/integration_test_report.json ]; then
              python -c "
          import json
          with open('/app/test-results/integration_test_report.json', 'r') as f:
              report = json.load(f)
          
          print(f\"Total Tests: {report['summary']['total_tests']}\")
          print(f\"Passed: {report['summary']['passed']}\")
          print(f\"Failed: {report['summary']['failed']}\")
          print(f\"Success Rate: {report['summary']['success_rate']}%\")
          print(f\"Duration: {report['summary']['duration']:.2f}s\")
          "
          fi
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
      volumes:
      - name: config-volume
        configMap:
          name: integration-testing-config
      - name: test-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: integration-test-service
  namespace: nexus-infrastructure
  labels:
    app: integration-test-runner
    component: testing
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP
  selector:
    app: integration-test-runner

